{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO71lMaPymu9Byksb6Kl31S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mukii21/GreenAI/blob/main/Untitled12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmqyNdQ1V767",
        "outputId": "25205dba-643f-41c4-c10a-e74d70603568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.2292 - loss: 0.6954  \n",
            "Epoch 2/5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9167 - loss: 0.6473\n",
            "Epoch 3/5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9167 - loss: 0.5919\n",
            "Epoch 4/5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7292 - loss: 0.5696\n",
            "Epoch 5/5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.4987\n",
            "Offensive Comments Detected:\n",
            "- You are so stupid and annoying.\n",
            "- Great work! Keep it up.\n",
            "- I hate you, you idiot!\n",
            "- You are such a loser.\n",
            "- Thank you for your help.\n",
            "- You are dumb and pathetic.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Sample data: Comments and their labels (1 for offensive, 0 for not)\n",
        "comments = [\n",
        "    \"You are so stupid and annoying.\",\n",
        "    \"Great work! Keep it up.\",\n",
        "    \"I hate you, you idiot!\",\n",
        "    \"This is a beautiful day.\",\n",
        "    \"You are such a loser.\",\n",
        "    \"Thank you for your help.\",\n",
        "    \"You are dumb and pathetic.\",\n",
        "    \"That was a fantastic performance!\",\n",
        "]\n",
        "labels = [1, 0, 1, 0, 1, 0, 1, 0]\n",
        "\n",
        "# Step 1: Text Preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove special characters and numbers\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
        "    return text\n",
        "\n",
        "# Preprocess all comments\n",
        "processed_comments = [preprocess_text(comment) for comment in comments]\n",
        "\n",
        "# Step 2: Tokenization and Padding\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(processed_comments)\n",
        "sequences = tokenizer.texts_to_sequences(processed_comments)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "# Pad sequences\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# Step 3: Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.25, random_state=42)\n",
        "\n",
        "# Step 4: Build the RNN Model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=64, input_length=max_length),\n",
        "    SimpleRNN(64, return_sequences=False),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 5: Train the Model\n",
        "model.fit(np.array(X_train), np.array(y_train), epochs=5, batch_size=2, verbose=1)\n",
        "\n",
        "# Step 6: Predict Offensive Comments\n",
        "def detect_offensive_comments(comments):\n",
        "    offensive_texts = []\n",
        "    for comment in comments:\n",
        "        processed_comment = preprocess_text(comment)\n",
        "        sequence = tokenizer.texts_to_sequences([processed_comment])\n",
        "        padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
        "        prediction = model.predict(padded_sequence, verbose=0)\n",
        "        if prediction[0][0] > 0.5:\n",
        "            offensive_texts.append(comment)\n",
        "    return offensive_texts\n",
        "\n",
        "# Step 7: Test the Model\n",
        "offensive_comments = detect_offensive_comments(comments)\n",
        "print(\"Offensive Comments Detected:\")\n",
        "for comment in offensive_comments:\n",
        "    print(\"-\", comment)\n"
      ]
    }
  ]
}